# KDP - Kafka - Flink - Pinot

This repo is designed to get the user introduced to the KDP stack.

## Abstract

A Kafka, Flink, and Pinot stack is a real-time streaming architecture that integrates these technologies to process and analyze data in near real-time. Kafka acts as a distributed streaming platform, ingesting data from various sources in real-time. Flink consumes data streams from Kafka topics, performing real-time stream processing, transformations, and computations on the incoming data. The processed data from Flink is then loaded into Pinot, a real-time distributed online analytical processing (OLAP) data store designed for low-latency analytics on large-scale data streams.

 **Apache Kafka** is a distributed event streaming platform designed for high-throughput, fault-tolerant, and scalable data ingestion and storage. It acts as a central nervous system for real-time data pipelines, enabling producers to publish events to topics and consumers to subscribe and process them. Kafka’s key strengths include its durable commit log, which ensures data persistence, and its ability to handle millions of events per second with low latency. In this workshop, Kafka will serve as the ingestion layer, streaming raw data (e.g., user actions or sensor readings) to downstream systems like Flink and Pinot, providing a reliable foundation for real-time processing and analytics.

**Apache Flink** is an open-source stream processing framework renowned for its ability to perform stateful computations over both unbounded (streaming) and bounded (batch) data with low latency and exactly-once guarantees. Flink excels at real-time data transformation, aggregation, and enrichment, making it ideal for complex event processing tasks. Its unified architecture and rich APIs (including SQL, DataStream, and Table) offer flexibility for developers and analysts alike. In the demo, Flink will process Kafka streams—filtering, aggregating, or joining data in real time—before feeding the results to Pinot, showcasing its power in turning raw events into meaningful, processed datasets.

**Apache Pinot** is a real-time distributed OLAP (Online Analytical Processing) datastore built for ultra-low-latency analytics on large-scale streaming and batch data. It ingests data directly from Kafka topics, organizes it into columnar segments, and enables sub-second query responses even under high throughput. Pinot’s strengths lie in its ability to handle real-time ingestion, support complex SQL queries, and scale horizontally across clusters. In this workshop, Pinot will act as the analytics layer, consuming processed data from Flink and Kafka to power interactive dashboards or ad-hoc queries, demonstrating its capability to deliver instant insights to end users.

### Building the pieces

We have a couple of docker builds.  

- Producer: is a python script that generates records for Player Scores.  
- Flink: composes all the flink tools needed for our transforms.

Let's start with building these first.

``` sh
docker-compose build
```

This will create two Docker images, one each for the producer and one for Flink.

### Startthe environment

Now, let's launch the individual components:

#### For Pinot

- Zookeeper
- Pinot Controller
- Pinot Broker
- Pinot Server

#### For Kafka

- will leverage the common zookeeper
- Kafa
- Red Panda for monitoring
- Producer for generating data to populate the Kafka streams

#### For Flink

- SQL Query consol for Flink
- Jobmanager for Flink
- Taskmanager for Flink

```sh
docker-compose up -d
```
### Create Kafka Topics

Now that we have the environments up, let's create some Kafka Topics

We will need four of these:

- Games
- Players
- Player Scores
- Enhanced Player Scores

``` sh
docker exec kafka kafka-topics.sh \
		--bootstrap-server localhost:9092 \
		--create \
		--topic games

docker exec kafka kafka-topics.sh \
		--bootstrap-server localhost:9092 \
		--create \
		--topic players

docker exec kafka kafka-topics.sh \
		--bootstrap-server localhost:9092 \
		--create \
		--topic player_scores

docker exec kafka kafka-topics.sh \
		--bootstrap-server localhost:9092 \
		--create \
		--topic player_scores_enhanced
```
### Populate Kafka Topics

Let's run the producer now, to start poulating the kafka topics.

``` sh
docker-compose run producer
```
This will start creating and sending messages to the kafka topics.  It will populate the Players and games irst, then create player scores.  Note that all games and all players will be pupulted since that is a small set.  You can limit the number of messages generated by adjusting the LIMIT environment variable for the Producer service in the docker-compse.yml file.

You can verify that the topics are being populated by checking at the Red Panda console [http://localhost:9080](http://localhost:9080).

### Create Tables in Pinot

We now have Kafka topic created and populated, let's ingest that into pinot.  For this, we will create some tables.  Three out of four will be populated directly from Kafka, and we will leave the last one for some Flink Magic.  

- Players table
- Games table
- PlayerSocre table
- PlayerscoreEnhanced -> waiting for Flink magic!

``` sh
docker exec pinot-controller ./bin/pinot-admin.sh \
		AddTable \
		-tableConfigFile /tmp/pinot/config/games.table.json \
		-schemaFile /tmp/pinot/config/games.schema.json \
		-exec

docker exec pinot-controller ./bin/pinot-admin.sh \
		AddTable \
		-tableConfigFile /tmp/pinot/config/players.table.json \
		-schemaFile /tmp/pinot/config/players.schema.json \
		-exec

docker exec pinot-controller ./bin/pinot-admin.sh \
		AddTable \
		-tableConfigFile /tmp/pinot/config/playerscores.table.json \
		-schemaFile /tmp/pinot/config/playerscores.schema.json \
		-exec

docker exec pinot-controller ./bin/pinot-admin.sh \
		AddTable \
		-tableConfigFile /tmp/pinot/config/playerscoresenhanced.table.json \
		-schemaFile /tmp/pinot/config/playerscoresenhanced.schema.json \
		-exec
```
You can check the data being populated by checking in the Pinot portal: [http://localhost:9000] and navigate to Query windows.

### Flink

Let's do some Flink!

To run the Flink SQL interface, 

```sh
docker-compose run sql-client
```

You should be able to create the Flink tables here. 

``` SQL

--- create Players table

CREATE TABLE Players
(
    player_id             INT,
    player_name           STRING,
    player_country        STRING,
    player_age            INT,
    player_created_time   TIMESTAMP(3)
) WITH (
      'connector' = 'kafka',
      'topic' = 'players',
      'properties.bootstrap.servers' = 'kafka:9092',
      'scan.startup.mode' = 'earliest-offset',
      'format' = 'json'
      );

-- validate data from PLayers table

SELECT *
FROM PLayers
WHERE LOWER(player_name) LIKE '%emma%';

-- Create Games table

CREATE TABLE Games
(
    game_id             INT,
    game_name           STRING,
    game_created_time   TIMESTAMP(3)
) WITH (
      'connector' = 'kafka',
      'topic' = 'games',
      'properties.bootstrap.servers' = 'kafka:9092',
      'scan.startup.mode' = 'earliest-offset',
      'format' = 'json'
      );

-- Validate data from Games table

SELECT * FROM Games
WHERE LOWER(game_name) LIKE '%rush%';

-- Create Player Scores

CREATE TABLE PlayerScores
(
    player_id             INT,
    game_id               INT,
    score                 INT,
    score_time            TIMESTAMP(3)
) WITH (
      'connector' = 'kafka',
      'topic' = 'player_scores',
      'properties.bootstrap.servers' = 'kafka:9092',
      'scan.startup.mode' = 'earliest-offset',
      'format' = 'json'
      );

-- Validate data freom player scores

SELECT player_id, game_id, AVG(score) as avgScore
FROM PlayerScores
GROUP BY player_id, game_id;

-- Create Enhanced PLayer Score table.

CREATE TABLE PlayerScoresEnhanced
(
    player_id     INT,
    player_name   STRING, 
    game_id       INT,
    game_name     STRING,
    score         INT,
    score_time    TIMESTAMP(3),  
    PRIMARY KEY (player_id, game_id) NOT ENFORCED -- Declare the PRIMARY KEY constraint
) WITH (
      'connector' = 'upsert-kafka', -- This enables updates and deletes
      'topic' = 'player_scores_enhanced',
      'properties.bootstrap.servers' = 'kafka:9092',
      'key.format' = 'json', -- Key format is JSON, matching the value format
      'value.format' = 'json' -- Values are serialized in JSON
      );

-- populate using joins.

INSERT INTO PlayerScoresEnhanced
    SELECT p.player_id,
           p.player_name,
           g.game_id,
           g.game_name,
           ps.score,
           ps.score_time
FROM PlayerScores ps
         JOIN
     Players p
     ON ps.player_id = p.player_id
         JOIN
     Games g
     ON ps.game_id = g.game_id;

-- Validate Data

SELECT player_name, game_name, AVG(score) as avgScore
FROM PlayerScoresEnhanced
GROUP BY player_name, game_name;

```

This last query is writing back to kafka.  At this point, pinot should be picking up the "Flinked" data.  To validate this, lets check:

- Kaka via red-panda consol [http://localhost:9080](http://localhost:9080) -> navigate to topics and playerscoresenhanced
- Pinot [http://localhost:9000](http://localhost:9000) -> navigate to queries and query PlayScoresEnhanced

## Tear Down

If you want to stop containers and remove the  images, you can run the following command:

``` sh
docker stop $(sudo docker ps -a -q)

docker rm $(sudo docker ps -a -q)

```